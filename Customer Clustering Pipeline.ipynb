{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients clustering algorithm\n",
    "# Disclaimer: This is not the notebook, where we did all the steps. We worked with smaller tasks in different environments, also we had to do 'save-restart kernel-load' very frequently as it consumes a lot of memory.\n",
    "# We just tried to reproduce the whole pipeline here\n",
    "# If you intersted in some of our files (customer embeddings or clusters labels) or mode details please contact us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pipeline description:\n",
    "# 1. Build customer buckets (list of favourite products in each category (lowest level of hierarchy)\n",
    "# 2. Use word2vec to obtain products embeddings (texts = customer buckets, words = products)\n",
    "# 3. Use tf-idf weighting of products embeddings to get customers (buckets) embeddings\n",
    "# 4. Use agglomerative clustering with cosine measure over customer embeddings to obtain customer clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from  tqdm import tqdm\n",
    "\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = pd.read_csv('clients.csv')\n",
    "plants = pd.read_csv('plants.csv')\n",
    "materials = pd.read_csv('materials.csv')\n",
    "transactions = pd.read_parquet('transactions.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop huge hash labels and replace them with indexes (3 times less storage consumed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = (transactions.merge(clients[['client_id']]\n",
    "                                   .reset_index(), how='left')\n",
    "                .drop(columns=['client_id'])\n",
    "                .rename(columns={'index':'client_id'}))\n",
    "transactions = (transactions.merge(plants[['plant']]\n",
    "                                   .reset_index(), how='left')\n",
    "                .drop(columns=['plant'])\n",
    "                .rename(columns={'index':'plant'}))\n",
    "transactions = (transactions.merge(materials[['material']]\n",
    "                                   .reset_index(), how='left')\n",
    "                .drop(columns=['material'])\n",
    "                .rename(columns={'index':'material'}))\n",
    "transactions['client_id'] = transactions['client_id'].fillna(-1).astype('int')\n",
    "transactions['material'] = transactions['material'].fillna(-1).astype('int')\n",
    "\n",
    "clients = (clients.reset_index().drop(columns='client_id')\n",
    "           .rename(columns={'index':'client_id'}))\n",
    "plants = (plants.reset_index().drop(columns=['plant'])\n",
    "          .rename(columns={'index':'plant'}))\n",
    "materials = (materials.reset_index().drop(columns=['material'])\n",
    "             .rename(columns={'index':'material'}))\n",
    "\n",
    "materials_hl2 = materials['hier_level_2'].unique()\n",
    "materials_hl2 = dict(zip(materials_hl2, range(materials_hl2.shape[0])))\n",
    "materials['hier_level_2'] = materials['hier_level_2'].map(materials_hl2)\n",
    "\n",
    "materials_hl3 = materials['hier_level_3'].unique()\n",
    "materials_hl3 = dict(zip(materials_hl3, range(materials_hl3.shape[0])))\n",
    "materials['hier_level_3'] = materials['hier_level_3'].map(materials_hl3)\n",
    "\n",
    "materials_hl4 = materials['hier_level_4'].unique()\n",
    "materials_hl4 = dict(zip(materials_hl4, range(materials_hl4.shape[0])))\n",
    "materials['hier_level_4'] = materials['hier_level_4'].map(materials_hl4)\n",
    "\n",
    "materials_vendor = materials['vendor'].unique()\n",
    "materials_vendor = dict(zip(materials_vendor, range(materials_vendor.shape[0])))\n",
    "materials['vendor'] = materials['vendor'].map(materials_vendor)\n",
    "\n",
    "transactions_check_ids = transactions['chq_id'].unique()\n",
    "transactions_check_ids = dict(zip(transactions_check_ids, range(transactions_check_ids.shape[0])))\n",
    "transactions['chq_id'] = transactions['chq_id'].map(transactions_check_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add total cheqs per client and total cheqs per plant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = (clients.merge(transactions[['client_id', 'chq_id']]\n",
    "                         .drop_duplicates()\n",
    "                         .groupby('client_id')[['chq_id']]\n",
    "                         .count()\n",
    "                         .rename(columns={'chq_id':'total_chqs_client'}).reset_index()))\n",
    "plants = (plants.merge(transactions[['plant', 'chq_id']]\n",
    "                       .drop_duplicates()\n",
    "                       .groupby('plant')[['chq_id']]\n",
    "                       .count()\n",
    "                       .rename(columns={'chq_id':'total_chqs_plant'}).reset_index()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter loyal clients (1 to 3 cheqs per week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = (transactions[['plant', 'client_id', 'chq_id']].merge(plants[['plant', 'total_chqs_plant']])\n",
    "                                                          .merge(clients[['client_id', 'total_chqs_client']]))\n",
    "filtered = filtered[(filtered['total_chqs_client']>=50)\n",
    "                    &\n",
    "                    (filtered['total_chqs_client']<=150)]\n",
    "\n",
    "transactions = transactions.merge(filtered[['client_id']].drop_duplicates(), how='inner')\n",
    "del filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building client buskets\n",
    "# The bucket is list of the most purchasable products for each category (1 product per category) (category = lowest level of hierarchy (we can potentially build the new analytical one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_client_basket(transactions, materials):\n",
    "    \n",
    "    tr1 = transactions[['client_id', 'material', 'sales_count']].groupby(['client_id', 'material']).sum().reset_index()\n",
    "    \n",
    "    unq_hier = tr1[['material']].drop_duplicates().merge(materials[['material', 'hier_level_4']])['hier_level_4'].unique()\n",
    "    dict_unq_hier = dict(zip(unq_hier, np.arange(len(unq_hier))))\n",
    "    \n",
    "    tr2 = pd.merge(tr1, materials[['material', 'hier_level_4']]).sort_values(['client_id', 'hier_level_4', 'sales_count'], ascending=False)\n",
    "\n",
    "    tr2.drop_duplicates(['client_id', 'hier_level_4'], inplace=True)\n",
    "    \n",
    "    dict_client_top_materials = {}\n",
    "    unq_client = tr2.client_id.unique()\n",
    "    \n",
    "    for client in tqdm(unq_client):\n",
    "        \n",
    "        tr3 = tr2[tr2.client_id == client]\n",
    "        lst = [-1] * len(unq_hier)\n",
    "        \n",
    "        for ind in range(tr3.shape[0]):\n",
    "            \n",
    "            sub_ind = dict_unq_hier[tr3.iloc[ind].hier_level_4]            \n",
    "            lst[sub_ind] = int(tr3.iloc[ind].material)\n",
    "\n",
    "        dict_client_top_materials[client] = lst\n",
    "        \n",
    "    return dict_client_top_materials\n",
    "\n",
    "buskets = collect_client_basket(transactions, materials)\n",
    "\n",
    "\n",
    "with open('buskets.pickle', 'wb') as handle:\n",
    "    pickle.dump(buskets, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing the baskets as the collection of texts\n",
    "# Recieved the list of ints(materials ids) for each customer, want to get a \"text\"(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[str(material) for material in client] for client in buskets.values()]\n",
    "texts_joined = [' '.join(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building material (word) embeddings\n",
    "# Lots of parameters to work with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 50\n",
    "WINDOW = len(sentences[0])\n",
    "ALGO = 1 # 1 for skip-gram, 1 for CBOW\n",
    "SAMPLING = 0 # 1 for hierarchical softmax, 0 for negative sampling\n",
    "NEGATIVE_SAMPLES = 5 # 5-20 for small datasets according to a papaer\n",
    "CBOW_MEAN = 1 # if 0 use sum not mean\n",
    "N_ITER = 5\n",
    "\n",
    "epoch_logger = EpochLogger()\n",
    "word2vec = Word2Vec(texts, size=EMB_SIZE, window=WINDOW, workers=8,\n",
    "                 min_count=1, sg=ALGO, hs=SAMPLING, negative=NEGATIVE_SAMPLES,\n",
    "                 max_vocab_size=None, cbow_mean=CBOW_MEAN, iter=N_ITER, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### want to get the embeddings in the form of the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {material: word2vec.wv.get_vector(material) for material in word2vec.wv.vocab.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building client (text) embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate tf-idf weights first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x.split())\n",
    "texts_tfidf = vectorizer.fit_transform(texts_joined).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack embeddings into the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_embedding_matrix = np.zeros((len(vectorizer.vocabulary_.keys()), EMB_SIZE))\n",
    "for product, vocab_idx in tqdm(vectorizer.vocabulary_.items()):\n",
    "    products_embedding_matrix[vocab_idx] = embeddings[product]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the client embeddings table as a matrix product of tf-idf matrix and word(product) embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_embedding = sentences_tfidf @ products_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='complete', distance_threshold=0.2)\n",
    "clusters_prediction = clustering.fit_predict(clients_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
